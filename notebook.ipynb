{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architecture\n",
    "\n",
    "This notebook can also be found [here](https://nbviewer.org/github/BoeJaker/Python-Neural-Networks/blob/master/notebook.ipynb)\n",
    "\n",
    "A neural network is a type of machine learning algorithm that is modeled after the structure of the human brain. It consists of layers of interconnected nodes, or neurons, that process information and output a result. Neural network architecture refers to the design and configuration of these layers, including the number of layers, the number of neurons in each layer, and the types of connections between them. \n",
    "\n",
    "There are many different configurations and methods for building neural networks, such as feedforward networks, recurrent networks, convolutional networks, and more. These different architectures and techniques are suited for different types of problems and data, and choosing the right one can greatly improve the accuracy and efficiency of your model. \n",
    "\n",
    "In this Jupyter notebook, we will explore the basics of neural network architecture and examine various configurations and methods for building and training effective models.\n",
    "\n",
    "***\n",
    "\n",
    "## Contents\n",
    "\n",
    "### Architecture</br>\n",
    "[Perceptron (SLP)](#Perceptron)\n",
    "    </br>A perceptron is a type of artificial neural network that is used for binary classification </br>\n",
    "[Perceptron (MLP)](#MLP)\n",
    "    </br>A type of neural network made up of multiple perceptrons arranged in layers. It's commonly used for tasks like classification and prediction.</br>\n",
    "[Feedforward Neural Network](#)\n",
    "    </br>These neural networks are the simplest type of neural network where the information flows in one direction from input to output layer without any feedback.</br>\n",
    "[Convolutional Neural Networks (CNN)](#)\n",
    "    </br>Used for image recognition and analysis, and are characterized by the use of convolutional layers, pooling layers, and fully connected layers.</br>\n",
    "[Recurrant Neural network (RNN)](#RNN)\n",
    "    </br>Designed for processing sequential data, such as time-series or natural language processing (NLP) tasks. They are characterized by the use of feedback loops that allow the network to maintain a \"memory\" of past inputs. </br>\n",
    "[Long Short Term Memory (LSTM)](#LSTM)\n",
    "    </br>A type of RNN that can selectively remember or forget past inputs. They are commonly used for NLP tasks such as language translation or text summarization.</br>\n",
    "[Autoencoder Neural Networks](#)\n",
    "    </br>Used for unsupervised learning and feature extraction. They consist of an encoder network that maps the input to a lower-dimensional space, and a decoder network that maps the lower-dimensional representation back to the original input.</br>\n",
    "[Generative Adversarial Networks](#)\n",
    "    </br>Used for generative modeling and image synthesis. They consist of a generator network that generates fake images and a discriminator network that attempts to distinguish between the fake images and real images.</br>\n",
    "[Siamese Neural Networks](#)\n",
    "    </br>Siamese networks are used for tasks such as image comparison and similarity scoring. They consist of two identical neural networks that share the same weights and process two inputs independently before combining them to produce an output.</br>\n",
    "[Reinforcement Learning Neural Networks](#)\n",
    "    </br>Reinforcement learning networks are used for learning through trial and error. They consist of an agent that interacts with an environment and receives rewards or punishments for its actions. The network learns to optimize its actions to maximize its rewards over time.</br>\n",
    "[Modular Neural Network](#)\n",
    "    </br>These networks consist of multiple independent neural networks that can be combined to form a larger network. This allows for more efficient training and better performance on complex tasks.</br>\n",
    "[Spiking Neural Network](#)\n",
    "    </br> These networks are designed to simulate the behavior of biological neurons, where information is transmitted through spikes of electrical activity. They are commonly used for modeling biological systems and for applications such as robotics and control systems.</br>\n",
    "[Deep Belief Networks (DBN)](#)\n",
    "    </br>A type of generative neural network that use a stack of Restricted Boltzmann Machines (RBMs) to learn a hierarchical representation of the data. They are commonly used for feature extraction and unsupervised learning.</br>\n",
    "[Echo State Networks](#)\n",
    "    </br>These networks are a type of recurrent neural network that use a large reservoir of randomly connected neurons to process input data. The output of the network is determined by a linear combination of the reservoir neurons. They are commonly used for time-series prediction and control tasks.</br>\n",
    "[Capsule Networks](#)\n",
    "    </br>A type of neural network that use capsules instead of neurons as the basic processing unit. Each capsule represents an instantiation of a specific entity or feature, and the network learns to recognize objects by combining the output of multiple capsules. They are commonly used for image recognition tasks.</br>\n",
    "[Neural Turing Machines](#)\n",
    "    </br>These networks combine a neural network with an external memory system, allowing the network to learn algorithms and perform tasks that require long-term memory. They are commonly used for tasks such as program synthesis and language modeling.</br>\n",
    "[Attention Mechanism Networks](#)\n",
    "    </br>Attention mechanism networks use an attention mechanism to selectively focus on different parts of the input data when processing it. This allows the network to selectively attend to important features and ignore irrelevant ones. They are commonly used for NLP tasks such as machine translation and text summarization.</br>\n",
    "[Transformer Network](#)\n",
    "    </br>A type of neural network architecture commonly used in natural language processing (NLP) tasks, such as machine translation, language modeling, and text classification. It was introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017.</br>\n",
    "\n",
    "### Encoding\n",
    "[One-Hot](#)\n",
    "    </br>This is a technique that converts categorical variables into a set of binary features, where each feature corresponds to a unique category.</br>\n",
    "[Label](#)\n",
    "    </br>This is a technique that assigns a unique integer to each category of a categorical variable.</br>\n",
    "[Binary Encoding](#)\n",
    "    </br>This is a technique that converts categorical variables into binary features, where each feature represents a different category, and only one feature is active (set to 1) for each sample. </br>\n",
    "[Count Encoding](#)\n",
    "    </br>This is a technique that replaces each category with the count of its occurrences in the dataset.</br>\n",
    "[Continuous](#)\n",
    "    </br>   \n",
    "[Circular](#)\n",
    "    </br>If the time of day is an important feature, it can be encoded using a variety of methods. One common approach is to use circular encoding, where the time of day is mapped onto a circle (e.g. 12:00pm is at the top of the circle, 6:00am is at the bottom) to capture the cyclical nature of time.</br>\n",
    "[Embedding](#)\n",
    "    </br>This is a technique that maps categorical variables to a low-dimensional vector space, where each category is represented by a vector.</br>\n",
    "[Scaling and Normalisation](#)\n",
    "    </br>Scaling and Normalization: This is a technique that scales and normalizes continuous variables to ensure that they have a similar range and distribution.</br>\n",
    "[Binnig](#)\n",
    "    </br>This is a technique that discretizes continuous variables into a set of categories or bins.</br>\n",
    "[Feature Extraction](#)\n",
    "    </br>This is a technique that extracts relevant features from the input data and represents them as a set of input features for the neural network.</br>\n",
    "\n",
    "\n",
    "### Training Data\n",
    "\n",
    "#### Text Corpus\n",
    " [Common Crawl](#Training)\n",
    " </br>A vast corpus containing crawls of the entire internet</br>\n",
    " [Web Text](#)\n",
    " </br>A diy corpus of reddit posts</br>\n",
    " [Wikipedia](#)\n",
    " </br>The Online Encyclopedia</br>\n",
    "\n",
    "***\n",
    "\n",
    "## [Perceptron](Architectures/perceptron.py)\n",
    "\n",
    "A perceptron is a type of artificial neural network that is used for binary classification<>\n",
    "\n",
    "[this writeup can also be found here](https://boejaker.com/index.php/2023/03/16/how-to-build-a-simple-neural-network-in-python-using-numpy/)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Neural networks are a type of machine learning model that are designed to mimic the behavior of the human brain. They are used for a wide range of applications, from image and speech recognition to natural language processing and predictive analytics.\n",
    "\n",
    "In this section, i will show you how to write a simple neural network, known as a perceptron, in Python using the NumPy library. This neural network will consist of an input layer, and an output layer, with a sigmoid activation function. The following steps are the basic building blocks of all typical, modern, neural network architectures. It is the basis of all modern AI systems like chat-GPT, DALL-E, and co-pilot.\n",
    "\n",
    "### Step 1: Import NumPy\n",
    "\n",
    "First, we need to import the NumPy library, which provides support for large, multi-dimensional arrays and matrices, along with a wide range of mathematical functions. This is the only library we need.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the sigmoid activation function\n",
    "\n",
    "![sigmoid](assets/images/sigmoid.png)*A visual representation of a sigmoid function*\n",
    "</br>\n",
    "The sigmoid() function is a mathematical function that maps any input value to a value between 0 and 1, which is useful for modeling the behavior of neurons in a neural network.\n",
    "\n",
    "An activation function is a crucial step in both artificial and biological neural networks. It allows neurons to do more than simply output the input they receive. Instead, the activation function works in a way that is similar to the rate at which action potentials fire in the brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "\n",
    "def sigmoid(x,deriv=False):\n",
    "    if(deriv==True): \n",
    "        return x*(1-x)\n",
    "    else: \n",
    "        return 1/(1+np.exp(-x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the neural network architecture\n",
    "\n",
    "![Single Layer Perceptron](assets/images/perceptron.png)*Diagram of a single layer perceptron*\n",
    "</br>\n",
    "Our neural network will consist of an input layer with three nodes, and an output layer with one node. This is known as a single layer perceptron.\n",
    "\n",
    "The specific size and number of layers in a neural network depend on factors such as problem complexity, dataset size, and available resources. A general rule of thumb is to match the input and output layers to the data. Hidden layers between the two will be covered in the next article.\n",
    "\n",
    "Ultimately, the best architecture should be determined through experimentation to find the one that performs best for the specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = 3\n",
    "output_layer = 1\n",
    "\n",
    "\n",
    "# Input matrix, 4 entires each with 3 inputs\n",
    "X = np.array([  [1,0,0],\n",
    "                [0,0,1],\n",
    "                [0,1,0],\n",
    "                [1,0,1] ])\n",
    "    \n",
    "# Output set, 1 output per input entry            \n",
    "y = np.array([  [0],\n",
    "                [1],\n",
    "                [0],\n",
    "                [1]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Initialize the weights\n",
    "\n",
    "The weights are the parameters that the neural network will learn during training. We will seed a numpy random number generator with np.random.seed(1), then initialize the weights randomly using the np.random.random() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Initialize the array of weights randomly\n",
    "W0 = np.random.random((input_layer,output_layer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Define the forward propagation function\n",
    "\n",
    "The forward_propagate() propagation function computes the output of the perceptron for a given input. It does this by multiplying the input by the weights of the input layer, and applying the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(X):\n",
    "    L0 = X\n",
    "    L1 = sigmoid(np.dot(L0,W0))\n",
    "    return L1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Train the perceptron\n",
    "\n",
    "The train() function passes the input thorough the forward propagation function. Then it performs a series of calculations known as backpropagation, which is used to update the weights of the network based on the error between the predicted output and the target output. Backpropagation consists of three steps\n",
    "The layer one error is calculated by subtracting the predicted output (L1) from the target output (y).\n",
    "The layer one delta, or the gradients of the layer one error with respect to the weights of each layer are then computed.\n",
    "These deltas are then used to update the weights of the network in the direction of the gradient.\n",
    "\n",
    "This process is repeated for a specified number of epochs, or until the error is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    global W0\n",
    "    for iter in range(10000):\n",
    "\n",
    "        # Forward propagation\n",
    "        L0 = X\n",
    "        L1 = sigmoid(np.dot(L0,W0))\n",
    "\n",
    "        # Calculate the difference between the predicted output (L1) \n",
    "        #  and target output (y)\n",
    "        L1_error = y - L1\n",
    "\n",
    "        # Multiply how much we missed by the slope of the sigmoid \n",
    "        #  at the values in L1\n",
    "        L1_delta = L1_error * sigmoid(L1,True)\n",
    "\n",
    "        # Update weights using the delta\n",
    "        W0 += np.dot(L0.T,L1_delta)\n",
    "\n",
    "    print(\"Output After Training:\")\n",
    "    print(L1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Test the perceptron\n",
    "To test our perceptron, we need to execute the train function, then we can pass a sample input through the forward_propagate() function and print the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result variable will be a value between 0 and 1, which represents the perceptorn’s prediction for the output. For clarity we have chosen to round the output to either 0 or 1 in the print() statement.\n",
    "\n",
    "This test results in the output [[1.]], which is the correct output as per the training data.\n",
    "\n",
    "## The Complete Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "imput_layer = 3\n",
    "output_layer = 1\n",
    "\n",
    "# Input matrix, 4 entires each with 3 inputs\n",
    "X = np.array([  [1,0,0],\n",
    "                [0,0,1],\n",
    "                [0,1,0],\n",
    "                [1,0,1] ])\n",
    "    \n",
    "# Output set, 1 output per input entry            \n",
    "y = np.array([  [0],\n",
    "                [1],\n",
    "                [0],\n",
    "                [1]])\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x,deriv=False):\n",
    "    if(deriv==True): \n",
    "        return x*(1-x)\n",
    "    else: \n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# Initialize the array of weights randomly\n",
    "W0 = np.random.random((imput_layer,output_layer))\n",
    "\n",
    "# Define our forward propogation function\n",
    "def forward_propagate(X):\n",
    "    L0 = X\n",
    "    L1 = sigmoid(np.dot(L0,W0))\n",
    "    return L1\n",
    "\n",
    "def train():\n",
    "    global W0\n",
    "    for iter in range(10000):\n",
    "\n",
    "        # Forward propagation\n",
    "        L0 = X\n",
    "        L1 = sigmoid(np.dot(L0,W0))\n",
    "\n",
    "        # Calculate the difference between the predicted output (L1) \n",
    "        #  and target output (y)\n",
    "        L1_error = y - L1\n",
    "\n",
    "        # Multiply how much we missed by the slope of the sigmoid \n",
    "        #  at the values in L1\n",
    "        L1_delta = L1_error * sigmoid(L1,True)\n",
    "\n",
    "        # Update weights using the delta\n",
    "        W0 += np.dot(L0.T,L1_delta)\n",
    "\n",
    "    print(\"Output After Training:\")\n",
    "    print(L1)\n",
    "\n",
    "train()    \n",
    "\n",
    "result = np.round(forward_propagate(np.array([[1,0,1]])))\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## [LSTM](Architectures/lstm.py)\n",
    "\n",
    "![LSTM Architecture](assets/images/lstm.png)*A high level diagram of an LSTM architecture*\n",
    "\n",
    "### Introduction:\n",
    "This section will provide an overview of a neural network code written in Python. The code includes a function for computing the sigmoid nonlinearity, a function for converting the output of the sigmoid function to its derivative, and a training dataset generation. It also initializes neural network weights, defines input variables, and has training logic to train the network on a simple addition problem.\n",
    "\n",
    "### Importing Libraries:\n",
    "The first step in the code is to import two libraries, \"copy\" and \"numpy\". These libraries are required for various functions in the code. The numpy library is used to work with arrays and mathematical operations, while the copy library is used for copying objects.\n",
    "\n",
    "### Sigmoid Function:\n",
    "The sigmoid function is a common activation function used in neural networks. It converts any input value to a value between 0 and 1. The sigmoid function defined in this code takes a parameter \"x\" and applies the formula 1 / (1 + exp(-x)) to it. The result is returned as the output of the function.\n",
    "\n",
    "### Sigmoid Output to Derivative Function:\n",
    "The sigmoid output to derivative function defined in this code takes a parameter \"output\", which is the output value of the sigmoid function applied to some input. The function then returns the derivative of the sigmoid function applied to that output value.\n",
    "\n",
    "### Training Dataset Generation:\n",
    "The next step in the code is to generate a training dataset. It does so by creating a binary dictionary of numbers and then generates two random numbers within a range of half the largest number. It then uses binary encoding to represent these numbers and generates their sum. These binary numbers are used to train the neural network to predict their sum.\n",
    "\n",
    "### Input Variables:\n",
    "The input variables defined in this code include alpha, input_dim, hidden_dim, and output_dim. Alpha represents the learning rate of the neural network. Input_dim represents the number of input neurons, hidden_dim represents the number of hidden neurons, and output_dim represents the number of output neurons.\n",
    "\n",
    "### Initialize Neural Network Weights:\n",
    "The code initializes the neural network weights using a random function from the numpy library. The weights are initialized for the input-hidden, hidden-output, and hidden-hidden layers of the neural network.\n",
    "\n",
    "### Training Logic:\n",
    "The code then enters the training logic, which iterates 10,000 times. It generates two random numbers and their binary representations, computes their sum, and stores their binary representation as the true answer. It then initializes the variable \"overallError\" to 0 and initializes two lists, \"layer_2_deltas\" and \"layer_1_values\", to store the error in the output layer and the hidden layer values, respectively.\n",
    "\n",
    " It then moves along the binary representation of the numbers, generating input and output values for each position. It computes the hidden layer values and the output layer values using the sigmoid function. It then calculates the error in the output layer and stores it in the \"layer_2_deltas\" list. It also computes the binary value of the output and stores it in the variable \"d\". It then stores the hidden layer values in the \"layer_1_values\" list for use in the next iteration.\n",
    "\n",
    "After the binary digits have been iterated through, the code enters the backpropagation step. It initializes the variable \"future_layer_1_delta\" to 0 and iterates through the binary digits in reverse order. For each digit, it computes the error in the output and the hidden layer, updates the weights, and stores the delta values for use in the next iteration.\n",
    "\n",
    "The weights are updated using the learning rate, the delta values, and the input and hidden layer values. The update values for each weight are stored in three separate update matrices, which are then added to the weight matrices. Finally, the update matrices are set to zero, and the code prints out the overall error and the predicted and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[3.45638663]\n",
      "Pred:[0 0 0 0 0 0 0 1]\n",
      "True:[0 1 0 0 0 1 0 1]\n",
      "9 + 60 = 1\n",
      "------------\n",
      "Error:[3.63389116]\n",
      "Pred:[1 1 1 1 1 1 1 1]\n",
      "True:[0 0 1 1 1 1 1 1]\n",
      "28 + 35 = 255\n",
      "------------\n",
      "Error:[3.91366595]\n",
      "Pred:[0 1 0 0 1 0 0 0]\n",
      "True:[1 0 1 0 0 0 0 0]\n",
      "116 + 44 = 72\n",
      "------------\n",
      "Error:[3.72191702]\n",
      "Pred:[1 1 0 1 1 1 1 1]\n",
      "True:[0 1 0 0 1 1 0 1]\n",
      "4 + 73 = 223\n",
      "------------\n",
      "Error:[3.5852713]\n",
      "Pred:[0 0 0 0 1 0 0 0]\n",
      "True:[0 1 0 1 0 0 1 0]\n",
      "71 + 11 = 8\n",
      "------------\n",
      "Error:[2.53352328]\n",
      "Pred:[1 0 1 0 0 0 1 0]\n",
      "True:[1 1 0 0 0 0 1 0]\n",
      "81 + 113 = 162\n",
      "------------\n",
      "Error:[0.57691441]\n",
      "Pred:[0 1 0 1 0 0 0 1]\n",
      "True:[0 1 0 1 0 0 0 1]\n",
      "81 + 0 = 81\n",
      "------------\n",
      "Error:[1.42589952]\n",
      "Pred:[1 0 0 0 0 0 0 1]\n",
      "True:[1 0 0 0 0 0 0 1]\n",
      "4 + 125 = 129\n",
      "------------\n",
      "Error:[0.47477457]\n",
      "Pred:[0 0 1 1 1 0 0 0]\n",
      "True:[0 0 1 1 1 0 0 0]\n",
      "39 + 17 = 56\n",
      "------------\n",
      "Error:[0.21595037]\n",
      "Pred:[0 0 0 0 1 1 1 0]\n",
      "True:[0 0 0 0 1 1 1 0]\n",
      "11 + 3 = 14\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "import copy, numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "\n",
    "# training dataset generation\n",
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "\n",
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "\n",
    "# input variables\n",
    "alpha = 0.1\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "# initialize neural network weights\n",
    "synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1\n",
    "synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1\n",
    "synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1\n",
    "\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)\n",
    "\n",
    "# training logic\n",
    "for j in range(10000):\n",
    "    \n",
    "    # generate a simple addition problem (a + b = c)\n",
    "    a_int = np.random.randint(largest_number/2) # int version\n",
    "    a = int2binary[a_int] # binary encoding\n",
    "\n",
    "    b_int = np.random.randint(largest_number/2) # int version\n",
    "    b = int2binary[b_int] # binary encoding\n",
    "\n",
    "    # true answer\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "    \n",
    "    # where we'll store our best guess (binary encoded)\n",
    "    d = np.zeros_like(c)\n",
    "\n",
    "    overallError = 0\n",
    "    \n",
    "    layer_2_deltas = list()\n",
    "    layer_1_values = list()\n",
    "    layer_1_values.append(np.zeros(hidden_dim))\n",
    "    \n",
    "    # moving along the positions in the binary encoding\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        # generate input and output\n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        # hidden layer (input ~+ prev_hidden)\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
    "\n",
    "        # output layer (new binary representation)\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # did we miss?... if so, by how much?\n",
    "        layer_2_error = y - layer_2\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2))\n",
    "        overallError += np.abs(layer_2_error[0])\n",
    "    \n",
    "        # decode estimate so we can print it out\n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
    "        \n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))\n",
    "    \n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "    \n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        X = np.array([[a[position],b[position]]])\n",
    "        layer_1 = layer_1_values[-position-1]\n",
    "        prev_layer_1 = layer_1_values[-position-2]\n",
    "        \n",
    "        # error at output layer\n",
    "        layer_2_delta = layer_2_deltas[-position-1]\n",
    "        # error at hidden layer\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        # let's update all our weights so we can try again\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "    \n",
    "\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha    \n",
    "\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "    \n",
    "    # print out progress\n",
    "    if(j % 1000 == 0):\n",
    "        print( \"Error:\" + str(overallError))\n",
    "        print( \"Pred:\" + str(d))\n",
    "        print( \"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        print( str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>***\n",
    "\n",
    "## Training\n",
    "\n",
    "\n",
    "### Common Crawl Requests\n",
    "\n",
    "Common Crawl is a web corpus that contains a vast amount of text data in multiple languages. The corpus is created by continuously crawling the internet and indexing the text from web pages, making it a valuable resource for training language models. Because Common Crawl contains text from a diverse range of sources, it provides a broad and varied sample of natural language, which is essential for training language models that can understand and generate human-like text. Additionally, because Common Crawl is freely available and accessible to researchers and developers worldwide, it has become a popular resource for training language models, with many state-of-the-art models using the dataset as a basis for their training.\n",
    "\n",
    "\n",
    "### [Common Crawl Wet Requests](Training%20Data/wet_requests.py)\n",
    "\n",
    " WET files are a type of web archive format used by Common Crawl to store text content from web pages. The script uses the requests library to download a list of WET file paths for the March 2023 crawl from Common Crawl's website. It then loops over each file path, downloads the corresponding WET file, decompresses it, and extracts its content using the warcio library. Finally, it decodes and prints the contents of the first three records (this can be changed) in each WET file. This code can be useful for researchers and developers who want to extract text data from Common Crawl for use in natural language processing or machine learning applications. \n",
    "\n",
    "The [code](Training%20Data/wet_requests.py) below imports the necessary libraries to download and process the web crawl data. It then sets the URL of the WET file paths for the March 2023 crawl, which contains the web crawl data in a compressed format.\n",
    "\n",
    "Next, it downloads the list of WET file paths using the requests library and decompresses the file using gzip. The decompressed file contains a list of URLs of individual WET files.\n",
    "\n",
    "After splitting the file content into individual file paths, the code loops over each file path, downloads the corresponding WET file, and prints its contents. The warcio library is used to create a WARC iterator, which iterates over the records in the WET file. The contents of the first three records are printed for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawl-data/CC-MAIN-2023-06/segments/1674764494826.88/wet/CC-MAIN-20230126210844-20230127000844-00000.warc.wet.gz\n",
      "Software-Info: ia-web-commons.1.1.10-SNAPSHOT-20230123022639\n",
      "Extracted-Date: Thu, 09 Feb 2023 17:39:18 GMT\n",
      "robots: checked via crawler-commons 1.4-SNAPSHOT (https://github.com/crawler-commons/crawler-commons)\n",
      "isPartOf: CC-MAIN-2023-06\n",
      "operator: Common Crawl Admin (info@commoncrawl.org)\n",
      "description: Wide crawl of the web for January/February 2023\n",
      "publisher: Common Crawl\n",
      "\n",
      "\n",
      "Изменение цвета кальмара для камуфляжа заснято на видео\n",
      "НЕ ПРОПУСТИ\n",
      "Экс-президент Польши заявил об «уникальном шансе разобраться с Россией»\n",
      "Названо оружие НАТО, способное долететь до Москвы и Санкт-Петербурга\n",
      "Бизнесмен Пригожин обратился к Володину с просьбой ввести уголовную ответственность за дискредитацию участников боевых действий — Блокнот Россия\n",
      "Депутатам Госдумы усложнили отдых за границей\n",
      "«Я кровь не останавливаю, а пускаю её врагам»: Пригожин назвал различие между собой и Распутиным\n",
      "Борис Джонсон неожиданно приехал на Украину, встретился с Зеленским и посетил Бучу\n",
      "Беглов формирует свой личный «общак» — Блокнот Россия\n",
      "Медведев назвал конфликт с Западом и Украиной новой Отечественной войной\n",
      "«У меня к вам есть вопрос»: Евгений Пригожин написал письмо в Белый дом\n",
      "«Свора кастрированных псов»: Медведев предупредил о появлении нового альянса против США\n",
      "НОВОСТНОЙ ЖУРНАЛ\n",
      "Главная\n",
      "АВТО\n",
      "НАУКА\n",
      "ЗДОРОВЬЕ\n",
      "КУЛЬТУРА\n",
      "ПОЛИТИКА\n",
      "СПОРТ\n",
      "ФИНАНСЫ\n",
      "ЭКОНОМИКА\n",
      "Главная » Общество » Изменение цвета кальмара для камуфляжа заснято на видео\n",
      "Изменение цвета кальмара для камуфляжа заснято на видео\n",
      "Группа исследователей из Окинавского института науки и технологий засняла, как осьминог меняет цвет на камуфляж, пишет burumbon.ru. Этот механизм позволяет морским обитателям прятаться от хищников. Во время эксперимента кальмары содержались в чистой части аквариума.\n",
      "Цвет кожи в таких условиях был светлым, но при наличии водорослей тела кальмаров начинали темнеть. Учёные установили, что такой способностью обладают кальмары и каракатицы, но впервые эта черта была замечена у кальмаров.\n",
      "По словам исследователя Рюты Накадзимы, кальмаров чаще всего можно увидеть в открытом океане. В новой работе ученые хотели выяснить, что происходит, когда кальмар находится рядом с коралловым рифом или когда приближается хищник.\n",
      "2022-04-07\n",
      "Иван\n",
      "Предыдущий Боднарчук высказался об Ани Лорак: «Его совесть не грызет! Нашей дружбе с ней пришел конец»\n",
      "Следующий РФРИТ объявил о запуске нового набора правообладателей программы цифровизации МСП\n",
      "Читайте также\n",
      "Марсоход NASA Perseverance побил рекорд в 318 метров в день\n",
      "14.04.2022\n",
      "Телеведущая Татьяна Лазарева рассмеялась известию о ее смерти\n",
      "12.04.2022\n",
      "РФРИТ объявил о запуске нового набора правообладателей программы цифровизации МСП\n",
      "09.04.2022\n",
      "Оставить комментарий Отменить написание\n",
      "Вы должны быть залогиненыдля комментирования\n",
      "Экс-президент Польши заявил об «уникальном шансе разобраться с Россией»\n",
      "Названо оружие НАТО, способное долететь до Москвы и Санкт-Петербурга\n",
      "Бизнесмен Пригожин обратился к Володину с просьбой ввести уголовную ответственность за дискредитацию участников боевых действий — Блокнот Россия\n",
      "Депутатам Госдумы усложнили отдых за границей\n",
      "«Я кровь не останавливаю, а пускаю её врагам»: Пригожин назвал различие между собой и Распутиным\n",
      "СЕЙЧАС ЧИТАЮТ\n",
      "В Гарварде создали мягкие роботизированные перчатки\n",
      "29.01.2020\n",
      "Овечкин отправил арбитра в больницу\n",
      "12.12.2019\n",
      "Число банкротств выросло почти в полтора раза\n",
      "27.06.2019\n",
      "Свитолина проиграла Пегуле и вылетела с Australian Open\n",
      "16.02.2021\n",
      "Медведев дал поручения вице-премьерам и восьми министерствам\n",
      "22.02.2019\n",
      "Новости в картинках\n",
      "КАЛЕНДАРЬ\n",
      "Январь 2023\n",
      "Пн\n",
      "Вт\n",
      "Ср\n",
      "Чт\n",
      "Пт\n",
      "Сб\n",
      "Вс\n",
      "« Дек\n",
      "1\n",
      "2 3 4 5 6 7 8\n",
      "9 10 11 12 13 14 15\n",
      "16 17 18 19 20 21 22\n",
      "23 24 25 26 27 28 29\n",
      "30 31\n",
      "Страницы\n",
      "карта сайта\n",
      "обратная связь\n",
      "ФИНАНСЫ\n",
      "Все материалы на данном сайте взяты из открытых источников - имеют обратную ссылку на материал в интернете или присланы посетителями сайта и предоставляются исключительно в ознакомительных целях. Права на материалы принадлежат их владельцам. Администрация сайта ответственности за содержание материала не несет. Если Вы обнаружили на нашем сайте материалы, которые нарушают авторские права, принадлежащие Вам, Вашей компании или организации, пожалуйста, сообщите нам.\n",
      "© 2018. Все права защищены\n",
      "\n",
      "心探坊-微信导航—免费发布公众号、微信群、小程序，海量人脉，专属有效群二维码。\n",
      "首页\n",
      "公众号\n",
      "微信群\n",
      "人脉\n",
      "小程序\n",
      "发布信息\n",
      "心探坊\n",
      "微信号：XTCLUB-2021\n",
      "推荐群\n",
      "联系客服\n",
      "微信群号大全免费\n",
      "咸鲤日记\n",
      "兼职小妙招~\n",
      "中皓酒庄\n",
      "肺鱼思维小鱼数学\n",
      "云南老乡最值得关注\n",
      "启蒙教育steam科学\n",
      "首页 > 公众号 >\n",
      "群二维码\n",
      "群主二维码\n",
      "扫描二维码添加公众号\n",
      "278\n",
      "[公众号] 心探坊\n",
      "所属分类：\n",
      "热度： 1886\n",
      "所在地区：广州\n",
      "微信号：XTCLUB-2021\n",
      "简介：\n",
      "痴迷亲密关系研究，执着个人成长体验。号主小学姐：编辑出身，钟情心理学多年，现事小说写作与心理学研究。\n",
      "池玉\n",
      "暂无信息\n",
      "信息为会员发布，本站不承担任何内容的法律责任，如您发现有侵犯您权益的内容，请联系我们删除。\n",
      "版权声明隐私保护用户协议免责声明© 微信导航 all rights reserved.黑ICP备18006982号-6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "import warcio\n",
    "\n",
    "# Set the URL of the WET file paths for the March 2023 crawl\n",
    "url = 'https://data.commoncrawl.org/crawl-data/CC-MAIN-2023-06/wet.paths.gz'\n",
    "\n",
    "# Download the list of WET file paths\n",
    "response = requests.get(url)\n",
    "compressed_file = response.content\n",
    "\n",
    "# Decompress the file\n",
    "file_content = gzip.decompress(compressed_file)\n",
    "\n",
    "# Split the file content into individual file paths\n",
    "file_paths = file_content.decode().split()\n",
    "# Loop over each file path, download the corresponding WET file, and print its contents\n",
    "for path in file_paths:\n",
    "    # Construct the URL of the WET file\n",
    "    print(path)\n",
    "    wet_url = 'https://data.commoncrawl.org/' + path\n",
    "    \n",
    "    # Download the WET file\n",
    "    response = requests.get(wet_url)\n",
    "    compressed_file = response.content\n",
    "    \n",
    "    # Decompress the file\n",
    "    file_content = gzip.decompress(compressed_file)\n",
    "    \n",
    "    # Create a WARC iterator\n",
    "    records = warcio.ArchiveIterator(BytesIO(file_content))\n",
    "    \n",
    "    # Iterate over the records and print the contents of the first record\n",
    "    for index, record in enumerate(records):\n",
    "        print(record.content_stream().read().decode('utf-8','ignore'))\n",
    "        if index >= 2 :\n",
    "            # To keep the demo output brief, break out of the loops early\n",
    "            break \n",
    "\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the output of the common crawl wet archives is irregular, multilingual and captures elements as well as text. This data will require filtering before training is complete. \n",
    "Filtering could be achieved via another neural network designed to extract english text that is over a threshold length.\n",
    "\n",
    "After filtering the WET data using existing tools or custom scripts, you may want to further refine the data to meet your specific requirements. For example, you may want to exclude certain types of content, such as pages written in a particular language or containing specific keywords. Alternatively, you may want to prioritize certain types of content, such as news articles or blog posts.\n",
    "\n",
    "To achieve this level of filtering, you can use more advanced techniques such as machine learning classifiers or natural language processing (NLP) algorithms. These tools can help you identify patterns and extract relevant information from the text, such as sentiment, topic, or author. By applying these techniques, you can create a more targeted and high-quality dataset for training your neural network.\n",
    "\n",
    "Overall, the process of filtering the WET data involves a combination of manual and automated techniques, depending on your specific goals and resources. It requires careful planning and experimentation to find the right balance between relevance, quality, and scalability. However, with the right tools and techniques, you can create a powerful training dataset for your language model and unlock its full potential."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "The transformer model relies on a self-attention mechanism, which allows it to process input sequences in parallel, rather than sequentially like traditional recurrent neural networks (RNNs). This makes it more efficient and better suited for longer sequences.\n",
    "\n",
    "In a transformer, the input sequence is first embedded into a high-dimensional space, and then multiple layers of self-attention and feedforward neural networks are applied. The self-attention mechanism allows the model to weigh the importance of different parts of the input sequence when making predictions. The feedforward neural networks then transform the representations learned by the self-attention layers into a form that can be used for the final prediction.\n",
    "\n",
    "Transformers have shown state-of-the-art performance on a variety of NLP tasks and have been widely adopted in both research and industry."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
